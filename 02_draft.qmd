---
title: "An Actuary Walks into a War Room: Introducing the `{actuwar}` R Package"
bibliography: "ref.bib"
author: Miles D. Williams^[Denison University, Granville, OH USA. Contact at mdwilliams@denison.edu.]
date: "`r Sys.Date()`"
abstract: "Cunen et al. (2020) recently took the unconventional approach of using a model from actuarial science known as the inverse Burr to study variation in war fatalities. While they tout its usefulness, and show how it can be customized to accomodate covariates in a regression framework, they offer little in the way of guidance on how to use it. They also fail to provide a convenient programming interface to make their modeling approach accessible to conflict scholars. This is a problem, because overcoming the technical challenges associated with modeling war fatalities matters for testing normatively appealing but hottly contested theories such as the decline of war thesis. To fill the gap, first, background on the technical challenges of modeling war fatalities and the promised helpfulness of the inverse Burr model in overcoming these challenges are discussed. Second, practical guidance is offered for how an inverse Burr model of war fatalities might be specified both formally and by way of a new R package called `{invburreg}`, how it is estimated, and how its results should be summarized to capture the nuances that this model is able to capture. Some words of warning are also provided. With proper guidance and accessible tools, the hope is that this model might receive more widespread acceptance within the conflict literature, and in other areas of political science where similar data challenges exist."
format: 
  docx:
    reference-doc: word_docx_new_template.docx
    number-sections: false
    number-depth: 3
    fig-dpi: 500
    fig-align: center
execute: 
  echo: false
  warning: false
  message: false
---

***Keywords***: Conflict, Battle Deaths, Inverse Burr, Power-law

# Introduction

"An actuary walks into a war room" sounds like the beginning of joke, but the idea matters a great deal for studying war. Actuaries typically work for insurance companies or private firms, and their job is to model financial risk and ruin. That means they care about more than a single point estimate of the expected value of, say, an insurance claim. They care about the probability distribution over the possible claims the insured might make. Often, these claims follow a thick tailed distribution, and because insurance companies want to avoid financial ruin, they take the odds of extreme events seriously and want their actuaries to use the most rigorous methods possible.

When @cunen2020 recently used the inverse Burr distribution to model the sizes of international wars, they drew from the deep well of actuarial science. They even used a statistical package created specifically for the purpose of doing actuarial science—an R package called `{actuar}`. What does it mean to approach the study of war through the lens of an actuary? What theoretical framework and assumptions are implied? @cunen2020 say very little about the actuarial roots of their methodology. Though they provide resources to replicate their work, the focus of their study is not to offer guidance on using the inverse Burr, nor is it to provide a convenient programming interface for others to use.

This is a missed opportunity to showcase the potential on offer with Cunen et al.'s approach. It also puts the cart before the horse. Like all actuarial tools, the inverse Burr distribution is intended to solve a particular problem with data, and it is used by actuaries in a particular way. This study is motivated by the goal of better contextualizing the use of the inverse Burr for studying the risk of deadly wars, and providing guidance and resources for researchers interested in implementing this method. The study proceeds in two parts.

First, the problem @cunen2020 turned to actuarial science to solve—the heavy-tailed distribution of war fatalities—is discussed. This problem poses a technical challenge and is source of contention among scholars interested in testing macro trends in war's deadliness. The merits of assessing the risk of deadly wars are self-evident, and by extension, so are the merits of micro-founding methods used to calculate this risk.

Second, readers are introduced to the inverse Burr model and how it can be used in conflict research. When @cunen2020 applied the model to the problem of modeling war size, they did not just borrow it from actuarial science, they also innovated by showing that the model could be parameterized with covariates. However, their novel approach lacks a consistent and easily accessible user interface for researchers, or a full explanation of the method's strengths and weaknesses. To facilitate better accessibility, a new statistical package developed in the R programming language is introduced that allows users to estimate an inverse Burr regression model and summarize the results in ways consistent with standard modeling functions in the R programming language. However, because of the unique purpose for which the inverse Burr was intended, assessing its output requires a different approach than most political scientists are accustomed to. The model is ideal, not for generating a unique prediction for war size, but a conditional distribution of possible war sizes that may take on a wide variety of complex shapes. The proposed R package contains tools that make summarizing and visualizing these model outputs as user-friendly as possible.

As the world is witnessing an upsurge in conflicts, the need for appropriate analytical tools and the right intuitions for how to use them is timely and necessary. Quantifying the risk of conflict escalation is important not just to academics, but to policy-makers as well. @cunen2020 introduced a helpful tool to conflict scholars for achieving this goal, but for it to be of greatest utility to the field, it is essential that researchers have guidance on how to use it and convenient tools for doing so.

# War, the Power-law, and the Alternative

One of the foundational questions in the quantitative study of war centers on variation in war deaths. Why are some wars short-lived and result in only a few thousand fatalities, while others last years (even decades) and kill millions? Lewis Fry Richardson[@richardson1948; -@richardson1960] was among the first to approach this question using data. He was preceded in this effort by data collection done by @wright1942study, but Richardson was the first to bring "real statistical acumen," as Braumoeller [-@braumoeller2021trends, 274] put it, to the data the Wrights produced. One of the key observations Richardson made was the fact that many wars are quite small while only a few become especially deadly. In his 1948 work, Richardson noted, in particular, that the distribution of war deaths displays power-law-like behavior, meaning that the wars that are the most deadly are exceptionally huge.

In the decades since Richardson's work, questions about war started to focus more on dyadic correlates of international conflict rather than trends in war size. This changed after the publication of best-selling book, *Better Angels of Our Nature*, by @pinker2011. Pinker claimed is that international wars, among many forms of violence, have become less deadly over time. This argument, known as the decline of war thesis, is not original to Pinker [see @gaddis1986long], but his book popularized the idea, prompting critical responses from international relations scholars. Some, like @fazal2014dead, raised substantive concerns about using the decline in war deaths over time as evidence of war's disappearance. She contends that improvements in battlefield medicine alone can explain the decline in deaths rather than a decline in bellicosity. Others, like @braumoeller2019, raise methodological concerns about drawing inferences about trends in war deaths because of the power-law-like behavior of war fatalities.

The latter concern has drawn the most attention from researchers. Just like Richardson found decades ago, more recent and higher quality datasets such as the war series produced by the Correlates of War (CoW) project [@sarkeeswayman2010rw] display extreme variation in war size. Figure 1 shows the variation in total battle deaths recorded per war in the CoW data by the first year the war started. The data documents 95 international wars from 1816 to 2007, and it shows that most of the wars fought during the past two centuries were generally small, while a few were enormous. World War II, the deadliest war in the CoW dataset, had over 16 million recorded battle deaths. This makes up about 51.8% of all battle deaths in the data. World War I and World War II combined make up 78.6% of all battle deaths, and the top 10 deadliest wars account for nearly 95% of all battle deaths. This kind of distribution is far more extreme than a typical Pareto distribution where 20% of observations are responsible for 80% of outcomes.

```{r}
fig <- function(name) {
  knitr::include_graphics(
    here::here("_figs", name)
  )
}
fig("fig1.png")
```

Data that display such massive up-swings in magnitude require specialized statistical tools to analyze. Most recent studies use the classic power-law model to study variation in war size [@braumoeller2019; @cederman2003; @cedermanEtAl2011; @cirillo2016statistical; @clauset2017enduring; @clauset2018trends; @spagat2020decline; @spagat2018fundamental]. The model's name takes inspiration from its functional form, which specifies that the likelihood of seeing an event greater than size $x$ is inversely proportional to the size of the event raised to the power $\alpha > 0$:

$$
\Pr(X > x) \propto 1 / x^\alpha
$$ {#eq-powerlaw}

The parameter $\alpha$ is known as the power-law slope, and it determines the likelihood of extreme events. Bigger values mean the likelihood of extreme events is lower, while smaller values mean the likelihood of extreme events is higher.

In practice, the power-law has some unique behaviors that pose a challenge for fitting it to real world data and for drawing sensible statistical inferences. One of the most consequential is that if the $\alpha$ parameter is less than or equal to 3, finite variance for the variable $x$ cannot be guaranteed. Even worse, if $\alpha$ is less than or equal 2, a finite mean cannot be identified. This means a power-law fit for the data may yield a result that says the expected magnitude of the variable is statistically indistinguishable from infinity. As it happens, this behavior applies to war. Many recent studies estimate $\alpha$s with values less than 2, and some scholars, like @braumoeller2019, infer from this that extinction-level wars are not just possible but probable, with a higher likelihood than many assume.

In terms of practical challenges, most real-world data do not uniformly obey the power-law. This fact is noted by @clausetEtAl2009 who recommend identifying a minimum value of $x$ such that the data is power-law distributed for all $x \geq x_\text{min}$. While this seems like a compromise, @clausetEtAl2007 argue that in many studies, power-law behavior in the extreme tail of a thick-tailed distribution is of greatest concern to researchers. Some data truncation is a necessary trade-off for ensuring an accurate fit for the power-law for the largest events in the data.

This practical concern is also relevant for studying war. Using the CoW dataset used to produce Figure 1, Figure 2 shows the optimal fit for a power-law model to the battle death series. Consistent with best-practice, the appropriateness of the power-law fit for data is shown using a log-log plot where the observed values of battle deaths are shown on the x-axis and the empirical CDF, $\Pr(X > x)$, is shown on the y-axis. Based on the form of the power-law model, in a log-log plot you should see a negative linear correlation between $x$ and $\Pr(X > x)$. Sure enough, this is what we observe in the data. This is with the caveat, however, that we observe power-law behavior for all large $x \geq x_\text{min}$. Specifically, the parameters for the power-law model are $\alpha = 1.525$ and $x_\text{min} = 7,061$. While the power-law slope seems to make for a good linear fit for more extreme wars, and while the minimum war size where the power-law applies is small, because of the extreme distribution in war deaths nearly a majority of the wars in the data *cannot be explained* by the model (46.3% fall below $x_\text{min} = 7,061$).

```{r}
fig("fig2.png")
```

Most scholars who have used the power-law to study variation in the size of war accept some data loss as a reasonable trade-off for the ability to accurately fit the power-law for extreme conflicts. However, more recent work by @cunen2020 raises the concern that the conventional power-law model leaves substantial, and informative, variation in war unexplained. By removing observations, @cunen2020 worry that statistical precision is being needlessly sacrificed for the sake of making a square peg fit into a round hole.

While it may seem like methodological splitting hairs, conflicting findings in recent studies suggest that this point has bite. Using power-law models fit before and after World War II and an innovative bootstrapping procedure for statistical inference with power-law slopes, @braumoeller2019 fails to find evidence that wars before and after 1945 are drawn from different distributions. Other work yields similar conclusions \[@clauset2017enduring; @clauset2018trends\], but not all studies agree [@cunen2020]; @spagat2020decline\]. @cunen2020, who use their preferred inverse Burr model, find evidence of a statistically detectable change in the chances of deadly wars in the second half of the 20th century. Unlike work that relies on the classic power-law, their approach uses all of the data in the CoW war series to draw this conclusion, suggesting that inferences with the classic power-law might lack the statistical power to detect a real change in war fatalities.

To demonstrate, Figure 3 shows the same data as presented in Figure 2. The difference is that the inverse Burr model, suggested by @cunen2020 as an alternative for the power-law, is fit to the data. The inverse Burr is more complex than the classic power-law, allowing it to accommodate power-law behavior in the extreme tail of the distribution while also explaining variation in smaller events that fall below the $x_\text{min}$ threshold required to fit the classic power-law to data.

```{r}
fig("fig3.png")
```

The inverse Burr model has three parameters: a scale parameter $\mu$, and two shape parameters, $\alpha$ and $\theta$. A good way to think about these parameters is that $\mu$ captures the central tendency of the data, $\alpha$ captures the density of the data leading up to its central tendency, and $\theta$ captures the density of extreme values to the right of the data's central tendency. Confusingly, the last parameter, $\theta$, is approximately analogous to the power-law slope $\alpha$.

The $\Pr(X > x)$ as specified by the inverse Burr is given as:

$$
\Pr(X > x) = 1 - \left[\frac{(x/\mu)^\theta}{1 + (x / \mu)^\theta} \right]^\alpha
$$ {#eq-invburr}

where $x$, $\mu$, $\alpha$, and $\theta$ are all strictly positive. For very large $x$, the fact that $\theta$ captures power-law behavior can be seen by the following approximate specification:

$$
\Pr(X > x) \approx \alpha (\mu / x)^\theta.
$$ {#eq-largeinvburr}

This functional form offers more flexibility, which is on display in Figure 3. Rather than being restricted to fitting a linear relationship between $x$ and $\Pr(X > x)$ in log-log space, the inverse Burr can fit a quasi-concave and monotone curve, allowing it to capture both small values of $x$ as well as very large values in a thick-tailed distribution.

While @cunen2020 tout the inverse Burr as an unproblematic alternative to the power-law, it should be noted that it under-predicts the likelihood of the most extreme wars in the CoW battle death series—a fact that is clearly visible in Figure 3. This is a point that others have raised [@braumoeller2021trends], but it is premature to dismiss the inverse Burr simply for this reason, because there is one additional promised benefit of the inverse Burr as opposed to the classic power-law that may off-set this limitation: its ability to model the distribution of $x$ in a regression model framework.

In their study, @cunen2020 showcase this strength by parameterizing their inverse Burr model using both a binary indicator for the time period (before or after a particular cut-point) for the $\mu$ and $\theta$ parameters, as well as a continuous covariate (the average polity score among countries fighting a war) that is part of the $\mu$ parameter as well. In particular, they specify that:

$$
\mu_{L,i} = \mu_{L,0}\exp(\beta_L w_i) \quad \text{and} \quad \mu_{R,i} = \mu_{R,0}\exp(\beta_Rw_i)
$$ {#eq-cunenspec}

along with $\theta_L$, $\theta_R$ and a constant $\alpha$. The value $w_i$ represents average polity scores, and the $\beta$ parameters capture how democracy scores differently predict the central tendency $\mu$ before and after the specified cut-point.

The ability to incorporate covariates into the estimation of an inverse Burr model potentially off-sets its inferior performance in the extreme tail of the CoW conflict series. As @cunen2020 note, their parameterized model offers a superior fit for the data relative to a basic inverse Burr model that assumes constant $\mu$, $\alpha$, and $\theta$.

The significance of this approach goes beyond providing a better fit with greater statistical precision. @cunen2020 use it to offer affirmative evidence consistent with the decline of war thesis, and they provide substantive insight into the role of democracy in explaining variation in war size. First, using an innovative change-point algorithm, @cunen2020 identify 1950 as the best fitting cut-point in the CoW series. At that cut-point, the difference in inverse Burr fits before and after are statistically distinguishable, with the post-1950 data consistent with a decline in the chance of particularly large wars.

Second, @cunen2020 show that democracy has little correspondence with war size before 1950, but that democracy predicts smaller wars after 1950. This finding applies specifically to the central tendency $\mu$, and means that when the average democracy score among countries fighting a war increases, the scale of wars declines.

Cunen et al.'s finding with respect to democracy is truly unique and novel. But the real gift their approach offers to conflict scholars is their methodology for identifying this relationship. The ability to model war size, not just as a stationary series, or by fitting separate models to discrete periods, offers researchers greater flexibility in identifying nuanced relationships between factors of interest and the deadly potential of war.

This, however, is where Cunen et al.'s [-@cunen2020] study ends and the contribution of this one begins. If a tool like the inverse Burr model is to have more wide-spread acceptance among conflict scholars interested in testing theories about war size, more guidance is required than @cunen2020 offer in their paper. While they do have an online appendix and make their R code available for implementing their method, Cunen et al. offer little in the way of practical guidance or a convenient user interface for applied researchers. This study is motivated by the desire to fill this gap by offering these resources to conflict scholars. The next section offers a brief primer on the inverse Burr, and the one that follows offers an applied example for how to use the inverse Burr to study war.

# A Brief Primer on the Inverse Burr

Like many innovations in statistics, the inverse Burr distribution was introduced at various points in different fields, and called by different names. @kleiber2008guide offers a concise history, noting in particular the equivalence between the inverse Burr, as it is known in actuarial science [@kleiber2003statistical; @klugman2012loss], and the Dagum distribution as it is known in some corners of economics where, for decades, it was mostly limited to non-English speaking journals. The genesis of the inverse Burr, or the Dagum distribution in particular, was the need to identify a better distributional model for income and wealth than either the log-normal or Pareto distributions. Unsatisfied with these options, in the 1970s Camilo Dagum [-@dagum1977anm] proposed a three parameter distribution, which would become his namesake, that blended the Pareto distribution's ability to handle thick-tailed data and the log-normal's ability to capture an interior mode, or peak in the density function [@kleiber2008guide]. The flexibility of the form that became the Dagum/inverse Burr model is such that it not only can handle thick-tailed data with an interior mode, but it can also capture zero-mode data (more akin to the classic power-law).

The impact that the three inverse Burr parameters have on the form of the density function is demonstrated in the next three figures (see @eq-burrdens for the density function). In each, one of the parameters is changed while holding the others fixed. This helps to show how an increase in each changes the mode of the data and the thickness of the distribution, both up to the mode and after it.

First, consider Figure 4. It shows how the density function of the inverse Burr changes for $x \in [0, 10]$ where $\mu \in \{0.5, 1, 2, 4 \}$ and $\alpha = \theta = 2$. For this particular arrangement of parameters, as $\mu$ increases, so does the mode, or peak, of the data's density distribution. In fact, holding the other parameters constant at 2, $\mu$ is approximately equivalent to the mode of the data.

```{r}
fig("fig4.png")
```

Now, consider how the density function changes with $\alpha$. Figure 5 is like the last, but it now shows the density function holding $\mu = \theta = 2$ while $\alpha \in \{0.5, 1, 2, 4\}$. As $\alpha$ increases, the mode does as well, but notice that the rate of increase is smaller. Also notice that the density of the left-hand side of the distribution shows far more variation. When $\alpha$ is small, the left-hand side is very thick, but as $\alpha$ increases the thickness of the left-hand side shrinks to zero for small $x$. In short, greater $\alpha$ means a lower chance of small events.

```{r}
fig("fig5.png")
```

Finally, consider $\theta$. Figure 6 shows how the density function changes holding $\mu = \alpha = 2$ and where $\theta = \{0.5, 1, 2, 4\}$. Notice that as $\theta$ increases a few interesting things happen. First, the mode increases, meaning the most likely event becomes larger. At the same time, for the highest $\theta$ the thickness of the right-hand side shrinks, meaning that the chances of very large events goes down. The same is true for the left-hand side of the distribution. Notice, however, that for the smallest value of $\theta$, not only does the mode approach 0, the distribution actually lacks an interior mode. The density at $x = 0$ is $\infty$. Further, the right-hand side has a much thicker tail, meaning the likelihood of very large $x$ is also higher. This behavior shows that the inverse Burr is capable of capturing not just power-law behavior in the extreme tail, but also across an entire variable's distribution.

```{r}
fig("fig6.png")
```

These three examples do not begin to scratch the surface of the inverse Burr's flexibility, but they do offer a sample of what the three key parameters of the model imply for data. An increase to the scale parameter $\mu$ increases the mode of the data, an increase to the $\alpha$ parameter increases both the mode and shrinks the lower tail of the data, and an increase to $\theta$ increases the mode and shrinks the lower and upper tails of the data. This combination gives the inverse Burr a high degree of built-in flexibility, which is part of the rationale @cunen2020 offer for preferring it to the classic power-law for studying war. If a further set of examples were considered, one would quickly realize that the clean delineation between these parameters and the shape of the distribution has exceptions due to the non-linear specification of the model.

This flexible combination offers more than a subtle hint that modeling data using the inverse Burr requires attention to more than just the central tendency of the data. Camil Dagum's goal in producing the functionally equivalent Dagum distribution was to provide an appropriate fit for the whole of a distribution. He wanted to explain not just the position of the mode, but also the density of observations leading up to and following it.

As a consequence, parameterizing the inverse Burr with covariates requires paying close attention to how the model is specified and, then, to how its results are interpreted. Such a goal is more comprehensive than is the norm in political science. It also has the potential to be somewhat confusing for those being newly introduced to this approach. The next section therefore walks through an applied example for how to use the inverse Burr, using the CoW war series to illustrate. This section also includes R code for an associated package called `{actuwar}` for using the inverse Burr to model war.

# Using the Inverse Burr to Study War

This section introduces an R package called `{actuwar}` that allows users to estimate an inverse Burr regression model with a variety of covariates. The following sub-sections break the process down into discrete chunks: specification, estimation, and summarizing results. A word of warning when using the inverse Burr is also offered. Note that the results presented in this section are mainly for illustrative purposes. Any inferences drawn about war size should be interpreted with all due caution.

## Specification

As noted before, the inverse Burr has three parameters, and each can be specified as a function of covariates or left as a constant. Modeling these parameters can be done by applying a non-linear transformation to a linear weighted sum of covariates, just like more common generalized linear models. @cunen2020, for example, use the exponent, which is also the approach taken here. Each of the inverse Burr parameters is strictly positive, and using the exponent preserves this quality regardless of the values generated by the linear sum of model covariates.

Deviating somewhat from @cunen2020, a convenient way to express the specification of each of the parameters is as follows. For an inverse Burr model fit to an outcome variable $x_i$, let the density distribution for this function be given as:

$$
\text{Burr}^{-1}(x_i; \mu_i, \alpha_i, \theta_i) = \frac{\alpha_i \theta_i(x_i / \mu_i)^{\alpha_i\theta_i}}{x_i[1 + (x_i / \mu_i)^{\theta_i}]^{\alpha_i + 1}} 
$$ {#eq-burrdens}

where each of the parameters is specified as the exponent of a weighted sum of model covariates:

$$
\begin{aligned}
\mu_i & = \exp(\mathbf W ' \mathbf \beta) \\
\alpha_i & = \exp(\mathbf Y ' \mathbf \delta) \\
\theta_i & = \exp(\mathbf Z'\mathbf \gamma)
\end{aligned}
$$ {#eq-paramspecs}

It is possible for the sets of covariates used to model these parameters to be identical, or completely different. These covariates may be either discrete or continuous, and each includes a constant.

Consider a scenario where a researcher wants to condition the shape of the inverse Burr distribution for war deaths in the CoW conflict series on the basis of three factors: an indicator for the post-1950 period, the average of belligerent country polity scores at the time the war started, and the natural log of the pooled belligerent country populations at the time the war started.[^1] If one wanted to allow all three inverse Burr parameters to vary as a function of these factors, they might write:

[^1]: These variables originate from the Polity Project dataset [@marshalletal2017p] and population data comes from the National Military Capabilities dataset [@singer1987rcwd; @singeretal1972cdu].

$$
\begin{aligned}
\mu_i & = \exp[\beta_0 + \beta_1 \text{post}_i + \beta_2 \text{polity}_i + \beta_3 \log(\text{pop}_i)] \\
\alpha_i & = \exp[\delta_0 + \delta_1 \text{post}_i + \delta_2 \text{polity}_i + \delta_3 \log(\text{pop}_i)] \\
\theta_i & = \exp[\gamma_0 + \gamma_1 \text{post}_i + \gamma_2 \text{polity}_i + \gamma_3 \log(\text{pop}_i)]
\end{aligned}
$$ {#eq-warspec}

For analysis, it is possible to program such a regression model using software like R, which is the approach that @cunen2020 took in their own analysis. While the user could be left to either copy and augment the code made available by @cunen2020, this is a sub-optimal approach for most researchers using the R programming language. Most R users are initially trained in regression analysis using a specific user interface. Users give data to a function, like `lm()`, and represent the model they want to estimate by creating a formula object, using syntax like `outcome ~ var1 + var2`.

To bring the user interface for inverse Burr models more in line with this convention, the `{actuwar}` R package was created.[^2] It makes it possible to specify an inverse Burr regression with all the specificity this kind of model requires using syntax that is mostly consistent with basic R modeling functions. The below code snippet offers an example estimating the model formally specified above.

[^2]: The development version of the package can be accessed here: <https://github.com/milesdwilliams15/invburreg/tree/main>.

```{r}
#| eval: false
#| echo: true

## open the package and access the wars example data
library(actuwar)
data("wars") # open example CoW wars dataset with covariates

## fit an inverse Burr model to data
model_fit <- ibm(
  outcome = fat,
  mu = ~ post1950 + dem + pop,
  alpha = ~ post1950 + dem + pop,
  theta = ~ post1950 + dem + pop,
  data = wars
)

```

The main function for estimating an inverse Burr model is called `ibm()`, which has a similar interface to a function like `lm()` which is used to estimate a linear model in R. Reflecting the complexity of the inverse Burr model, however, the code uses a slightly modified syntax compared with `lm()`. There are three parameters that can accommodate covariates, and so there are three places in the `ibm()` function, to specify a parameter-specific formula. Note that, unlike typical approaches, only the right-hand side of the formula is required for each parameter. Since there can only be one outcome variable, the option `outcome` allows users to specify the outcome only once so that it does not have to be repeated for each parameter-specific formula. The object called `model_fit` is now the saved estimated inverse Burr model for war size.

## Estimation

Over the years, many different methods for fitting the inverse Burr to data have been proposed. Early efforts relied on non-linear least squares, but today, maximum likelihood estimation (MLE) is the recommended approach [@kleiber2008guide]. @cunen2020 use MLE in their own analysis of the CoW war data.

Other technical resources exist if readers are interested in the specifics of MLE for the inverse Burr [@dey2017dagum], but a short summary is offered here. Given an outcome $x_i$, the optimal values of $\hat \mu_i$, $\hat \alpha_i$, and $\hat \theta_i$ are those that maximize (expressed in log-form):

$$
L = \sum_{i = 1}^n \log[\text{Burr}^{-1}(x_i; \hat \mu_i, \hat \alpha_i, \hat \theta_i)]
$$ {#eq-mle}

As with most MLE methods, a closed form solution does not exist, so a numerical optimizer is required. @cunen2020 use the method developed by @nelder1965simplex, which is also the same method used by the `{actuwar}` package.

While estimation is straightforward, inference for inverse Burr parameters is somewhat fraught. @cunen2020 opt to use the Hessian to calculate parameter standard errors, but in practice this does not always work. In trials done by this author, there are certain combinations of model specifications and predictors that yield an indefinite Hessian, even though the @nelder1965simplex method was able to identify a unique solution. For this reason, the `{invburreg}` package relies on bootstrapping to compute standard errors. The trade-off is that while bootstrapping offers a consistent and robust estimate of parameter variance, it can be computationally demanding. Users can specify how many bootstrap iterations `ibm()` will implement as an option before estimating a model.

## Summarizing Results

There are several ways to summarize the results for an inverse Burr regression. Two approaches are presented here. The first is by way of a regression table. This is the format that most journals in political science expect to see regression results presented, and it is trivial to do for inverse Burr models. However, there are some important considerations to keep in mind.

For one, since covariates can be used to fit three different parameters, this will need to be signaled somehow in the table. It might be appropriate either to use row headings for model predictors, or to have multiple columns. In Table 1, the second approach is adopted. Results are shown for two different inverse Burr models. One is a *Baseline* model that includes no covariates, and the other is a *Covariate* model where a post-1950 indicator, average polity, and the log of pooled country population are used as predictors for all three inverse Burr parameters. There are six columns in total, two for each of these parameters with a label indicating which parameter estimates they are associated with. Note that the parameters are labelled as the log of their value. This is because the exponential transformation was used to transform the linear component of the model to ensure fitted parameter values are strictly positive.

```{r}
fig("fig_regtab.png")
```

While the presentation of the model in a regression table is more involved, many elements of the output remain familiar. Each cell entry is a coefficient estimate for the relevant parameter ($\alpha$, $\mu$, or $\theta$) and standard errors are shown in parentheses with statistical significance indicated using stars.

To access all this information necessary to construct a regression table, one can access a built-in model summary in an inverse Burr model object. As the below code example demonstrates, once a model is estimated, a model summary can be pulled out of the object using the `$` operator.

```{r}
#| echo: true
#| eval: false
library(actuwar)
data("wars") 

model_fit <- ibm(
  outcome = fat,
  mu = ~ post1950 + dem + pop,
  alpha = ~ post1950 + dem + pop,
  theta = ~ post1950 + dem + pop,
  data = wars
)

## look at the model summary
model_fit$out
```

```{r}
#| eval: true
#| echo: false
library(tidyverse)
library(invburreg)
data("wars") 

model_fit <- ibm(
  outcome = fat,
  mu = ~ post1950 + dem + pop,
  alpha = ~ post1950 + dem + pop,
  theta = ~ post1950 + dem + pop,
  data = wars
)

## look at the model summary
model_fit$out
```

Before turning to the next method of summarizing results, consider the estimates shown in Table 1. For this specification of the inverse Burr model for war fatalities, the only model parameter where covariates are statistically significant is $\alpha$. Recall from the previous section that this term captures both variation in the location of the mode and the thickness of the right-hand side of the distribution. An increase in $\alpha$ increases both the mode while reducing the likelihood of small events. Among the three factors included in the model, average democracy and pooled belligerent population are the only significant factors. Contrary to @cunen2020, the post-1950 indicator is not significant, though its sign is consistent with a lower mode and higher likelihood of small wars. Meanwhile, an increase in the pooled population of belligerents increases modal fatalities and reduces the chances of a smaller war. Surprisingly, democracy scores do as well, which conflicts with the findings of @cunen2020.

These conclusions are about as far as a regression table can take us. Because of the non-linear nature of the inverse Burr model, the results are better visualized. As will be shown, visualizing the data also reveals ways that the regression estimates when viewed on their own can be misleading.

The approach taken by the `{actuwar}` package is to simulate draws from a fitted inverse Burr distribution, holding covariates constant at quantities of interest, and then to show the results in a log-log plot similar to those presented in an earlier section. Draws from different configurations of covariates or different models can be compared in a single visualization. Figure 7 offers an example. Results for both the *Baseline* and the *Covariate* models are shown. For the latter, a distribution of war deaths is simulated holding covariates constant at their values for World War II. For reference, the empirical CDF for World War II is shown in the plot as well.

```{r}
fig("fig7.png")
```

The simulated inverse Burr distributions from these two models show that the *Covariate* model makes for a much better fit for World War II than the *Baseline* model. The former generates a distribution that substantially under predicts $\Pr(X > x)$ for World War II. Conversely, World War II could very easily have been drawn from the conditional inverse Burr distribution fit with covariates. This is one of the purported benefits of the inverse Burr model. It's built-in flexibility is enhanced by incorporating covariates.

This method of presenting model results can also be a good way to present the conditional impact of different covariates. Because of the non-linear nature of the inverse Burr, significant predictors in one model parameter can still impact variation in other parts of the distribution. This fact is on display in Figure 8, which shows simulated CDFs of war deaths based on the *Covariate* model. Holding the other factors constant at their mean, the average polity of belligerents is allowed to vary from -10, to 0, to 10.

```{r}
fig("fig8.png")
```

The figure shows that the overall shape of the fatality distribution changes conditional on polity, and in somewhat counter-intuitive ways. Consistent with the inference made when discussing the results in Table 1, an increase in the average polity scores among belligerents increases the mode of the distribution and appears to reduce the likelihood of smaller wars. At the same time, the upper tail of the distribution becomes thinner, and substantially so. That means that as the average polity score among belligerents increases, the likelihood of extremely huge wars declines. This is a finding that would have been missed if basing this inference exclusively on it's significance in fitting the $\theta$ parameter which controls the shape of the right-hand tail of the distribution.

Using `{actuwar}`, the code to produce a figure like the one above is straightforward. The function `ibm_sim()` will simulate draws from an inverse Burr model estimated with `ibm()`, and it can do holding certain model covariates fixed while allowing one or more to vary. To do this, a user can give `ibm_sim()` a fitted, and then specify a `newdata` object to pass to the function to use for simulating data. The interface is quite similar to the `predict.lm()` function used to get predictions from a linear model in R. The code below shows how to reproduce simulated data that was use to make Figure 8.

```{r}
#| eval: false
#| echo: true
ibm_sim(
  model_fit,
  newdata = tibble(
    post1950 = mean(wars$post1950),
    dem = c(-10, 0, 10),
    pop = mean(wars$pop)
  )
) -> dem_sim

```

## A Word of Warning

The inverse Burr model can be a helpful tool in the study of war. The previous sections make this clear. It offers a flexible distributional form that is amenable to analysis in a regression framework, and with the right tools it is easy to summarize model results to show the substantive impact of certain covariates on war deaths.

There is one major limitation worth mentioning, however. The model is sensitive to the scale of covariates as well as the outcome. Consider Table 2, which shows the results for the same *Covariate* model presented in Table 1 and a similar model using the very same data, with the caveat that war fatalities have been rescaled. The results are different. Following @cunen2020, two changes were made to fatalities. First, since no war in the data can have less than 1,000 total deaths, the data is shifted downward so that the smallest war is now of size 1. Second, to add extra variation to smaller wars (there are several that clock in at 1,000) they are recoded to have values between 1 and 9. This transformation, while having no real impact on the ordering of the fatality data, does influence the fit of the inverse Burr model. Seemingly innocuous transformations like this can add up to dramatically different results.

```{r}
fig("fig_regtab2.png")
```

Because the inverse Burr can be so sensitive to the scaling of the outcome, any transformations that a researcher wants to apply should be clearly justified. Even then, it may be advisable to report estimates with and without this desired transformation to check whether, and by how much, it changes the results.

# Conclusion

An actuary that enters a war room would bring a whole-of-distribution approach to assessing the fatality risk of war. This makes for a richer analysis, one that can capture unexpected and important nuances in the impact of different factors on this risk. While @cunen2020 brought these tools to the attention of conflict scholars, their study offers little in the way of practical guidance or intuitions for applied researchers. The goal behind this study was to introduce readers to the relevant tools for doing this kind of analysis and how to use them.

The particular actuarial tool suggested by @cunen2020, the inverse Burr, has substantial build-in flexibility. This gives it the ability to fit a wide range of distributions with power-law tails of varying thickness and with either a zero or interior mode. It also can handle covariates in a regression framework, where the researcher is free to allow any of the inverse Burr's three parameters to be conditioned by these factors.

An R package called `{actuwar}` was introduced that offers an intuitive user interface for political scientists interested in using this method to analyze correlates of war deaths. It adopts many coding conventions common to other models in the R programming language.

In addition, ways of specifying inverse Burr models, details about model estimation and inference, and presentation of results were addressed. A word of warning was provided as well. One potential trade-off with the inverse Burr's flexibility is its sensitivity to the scaling of the outcome variable. Researchers should do their best to ensure that any data transformations they apply are justified, and they may want to show results both with and without their preferred transformations to assure readers that their results are insensitive to them.

Having a tool like the inverse Burr for studying sizes of wars is a timely and important issue. Since @pinker2011 published *Better Angels of Our Nature*, a number of scholars have taken up the gauntlet of assessing the so-called decline of war thesis. The literature that has emerged around this issue yields mixed results and has engendered disagreement over modelling and measurement strategies [@braumoeller2019; @cederman2003; @cedermanEtAl2011; @cirillo2016statistical; @clauset2017enduring; @clauset2018trends; @cunen2020 @spagat2020decline; @spagat2018fundamental; @weisiger2013logics]. While the method proposed by @cunen2020 has met with some skepticism [@braumoeller2021trends], there is great potential for the inverse Burr to play a more prominent role in this literature and the overall debate surrounding the decline of war. In particular, its ability to accommodate covariates offers the possibility of testing nuanced hypotheses about various factors at the systemic level of analysis as they relate to war's escalatory potential.

The results shown in this study are primarily for demonstration, but they also show how the inverse Burr supports a richer analysis. Using the CoW conflict series, the parameters of an inverse Burr model of war fatalities were allowed to vary as a function of a post-1950 indicator, average belligerent polity scores, and the log of the pooled belligerent populations. The results show, first, that the inclusion of covariates improves the inverse Burr's performance, particularly in explaining extreme wars like World War II. Second, they show that the expected distribution of war sizes is sensitive to factors like democracy and population. With respect to democracy in particular, an increase in average polity scores predicts an increase in the modal war size and a decline in the likelihood of small wars. At the same time, it predicts a decline in the chances of huge wars. This unique finding might help explain why the decline of war thesis as been so hard to peg down. Polity scores have generally increased over time, and they seem to predict contradictory trends in the chances of small versus large wars.

It is the hope that conflict scholars will be able to use this method and the associated R package to better explain variation in war size and test a wide range of old and new theories about war escalation. But this method need not be restricted to this narrow sub-field. Political scientists studying a range of issues may find the inverse Burr a helpful tool for solving similar data challenges where easy solutions did not exist.

# References
