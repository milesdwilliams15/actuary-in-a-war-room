---
title: "An Actuary Walks into a War Room: Using a Model from Actuarial Science to Predict War Size"
bibliography: "ref.bib"
author: Miles D. Williams^[Denison University, Granville, OH USA. Contact at mdwilliams@denison.edu.]
date: "`r Sys.Date()`"
abstract: "Cunen et al. (2020) recently took the unconventional approach of using a model from actuarial science known as the inverse Burr to study variation in war fatalities. While they tout its usefulness, and show how it can be customized to accomodate covariates in a regression framework, they offer little in the way of guidance on how to use it. They also fail to provide a convenient programming interface to make their modeling approach accessible to conflict scholars. This is a problem, because overcoming the technical challenges associated with modeling war fatalities matters for testing normatively appealing but hottly contested theories such as the decline of war thesis. To fill the gap, first, background on the technical challenges of modeling war fatalities and the promised helpfulness of the inverse Burr model in overcoming these challenges are discussed. Second, practical guidance is offered for how an inverse Burr model of war fatalities might be specified both formally and by way of a new R package called `{invburreg}`, how it is estimated, and how its results should be summarized to capture the nuances that this model is able to capture. Some words of warning are also provided. With proper guidance and accessible tools, the hope is that this model might receive more widespread acceptance within the conflict literature, and in other areas of political science where similar data challenges exist."
format: 
  docx:
    reference-doc: word_docx_new_template.docx
    number-sections: false
    number-depth: 3
    fig-dpi: 500
    fig-align: center
execute: 
  echo: false
  warning: false
  message: false
---

***Keywords***: Conflict, Battle Deaths, Inverse Burr, Power-law

# Introduction

Risk is the business of actuaries, and when it comes to war, business is good. Actuaries typically work for insurance companies or firms, and their job is to model financial risk and ruin. That means they care about more than a single point estimate of the expected value of, say, an insurance claim. They care about the probability distribution over the possible claims the insured might make. Often, these claims follow a thick tailed distribution, and because insurance companies and firms want to avoid financial ruin, they take the odds of extreme events seriously and want their actuaries to use the most rigorous methods possible for modeling this risk.

When @cunen2020 recently used the inverse Burr distribution to model the sizes of international wars, they drew from the deep well of actuarial science. They even used a statistical package created specifically for the purpose of doing actuarial science – an R package called `{actuar}` – to apply the inverse Burr to the data. What does it mean to approach the study of war through the lens of an actuary? What theoretical framework and assumptions are implied? @cunen2020 say very little about the actuarial roots of their methodology. And though the provide resources to replicate their work, the focus of their study is not to offer guidance on using the inverse Burr, nor is it to provide a convenient programming interface for others to use.

This is a missed opportunity to showcase the potential on offer with Cunen et al.'s approach. It also puts the cart before the horse. Like all actuarial tools, the inverse Burr distribution is intended to solve a particular problem with data, and it is used by actuaries in a particular way. This study is motivated by the goal of better contextualizing the use of the inverse Burr for studying the risk of deadly wars, and providing guidance and resources for researchers interested in implementing this method. The study proceeds in two parts.

First, the problem @cunen2020 turned to actuarial science to solve – the heavy-tailed distribution of war fatalities – is discussed. This problem has posed a technical challenge and, thus, been a source of contention among scholars interested in testing macro trends in war's deadliness. Cunen et al.'s work is a recent contribution to a small but growing literature centered on the decline of war thesis. Much of this literature has, in part, developed in response to the best-selling book, *The Better Angels of Our Nautre,* by Steven @pinker2011. The merits of assessing the risk of deadly wars are self-evident, and by extension, so are the merits of micro-founding methods used to calculate this risk.

Second, readers are introduced to the inverse Burr model and how it can be used in conflict research. When @cunen2020 applied the model to the problem of modeling war size, they did not just borrow it from actuarial science, they also innovated by showing that the model could be parameterized with covariates. However, their novel approach lacks a consistent and easily accessible user interface for researchers, or a full explanation of what this actuarial tool implies for this methods appropriate use and evaluation. To facilitate better accessibility, a new statistical package developed in the R programming language is introduced that allows users to execute an inverse Burr regression model and summarize the results in ways consistent with standard modeling functions in the R language. However, because of the unique purpose for which the inverse Burr was intended, assessing its output requires a different approach than most political scientists are accustomed to. The model is ideal, not for generating a unique prediction for war size, but a conditional distribution of possible war sizes that may take on a wide variety of complex shapes.

As the world is witnessing an upsurge in conflicts, the need for appropriate analytical tools and the right intuitions for how to use them is timely and necessary. Quantifying the risk of conflict escalation is important not just to academics, but to policy-makers as well. @cunen2020 introduced a helpful tool to conflict scholars for achieving this goal, but for it to be of greatest utility to the field, it is essential that researchers have guidance on how to use it and convenient tools for doing so. This goes for other areas of political science where similar data challenges exist where the inverse Burr might offer a solution.

# War, the Power-law, and the Alternative

One of the foundational questions in the quantitative study of war centers on variation in war deaths. Why are some wars short-lived and result in only a few thousand fatalities, while others last years (even decades) and kill millions? @richardson1948 [-@richardson1960] was among the first to approach this question using data. He was preceded in this effort by data collection done by @wright1942study, but Richardson was the first to bring "real statistical acumen," as Braumoeller [-@braumoeller2021trends, 274] put it, to the data the Wrights produced. One of the key observations Richardson made was the fact that many wars are quite small while only a few become especially deadly. In his 1948 work, Richardson noted, in particular, that the distribution of war deaths displays power-law like behavior, meaning that the wars that are the most deadly are exceptionally huge.

This observation is preserved in more recent and higher quality datasets such as the war series produced by the Correlates of War (CoW) project [@sarkeeswayman2010rw]. Figure 1 shows the variation in total battle deaths recorded per war in the CoW data by the first year the war started. The data documents 95 international wars from 1816 to 2007, and, just like Richardson observed decades ago, this more recent dataset shows that most of the wars fought during the past two centuries were generally small, while a few were enormous. World War II, the deadliest war in the CoW dataset, had over 16 million recorded battle deaths. This makes up about 51.8% of all battle deaths in the data. World War I and World War II combined make up 78.6% of all battle deaths, and the top 10 deadliest wars account for nearly 95% of all battle deaths. This kind of distribution is far more extreme than a typical Pareto distribution where 20% of observations are responsible for 80% of outcomes.

```{r}
fig <- function(name) {
  knitr::include_graphics(
    here::here("_figs", name)
  )
}
fig("fig1.png")
```

Data that is prone to the kind of massive up-swings in magnitude seen in war require specialized statistical tools to analyze. While methods differ a bit, most recent studies use the classic power-law model to study variation in war size [@braumoeller2019; @cederman2003; @cedermanEtAl2011; @cirillo2016statistical; @clauset2017enduring; @clauset2018trends; @spagat2020decline; @spagat2018fundamental]. The model's name takes inspiration from its functional form, which specifies that the likelihood of seeing an event greater than size $x$ is inversely proportional to the size of the event raised to the power $\alpha > 0$:

$$
\Pr(X > x) \propto 1 / x^\alpha
$$ {#eq-powerlaw}

The parameter $\alpha$, what can also be called the power-law slope, determines the likelihood of extreme events. Bigger values mean the likelihood of extreme events is lower, while smaller values mean the likelihood of extreme events is higher.

In practice, the power-law has some unique behaviors and poses some challenges for fitting it to real world data. First, in terms of unique behaviors, one of the most consequential is that if the $\alpha$ parameter is less than or equal to 3, finite variance for the variable $x$ cannot be guaranteed, and if $\alpha$ is less than or equal 2, a finite mean cannot be identified. This means, depending on how extreme the distribution of the variable $x$ is, a power-law fit for the data may yield a result that says the expected magnitude of the variable is statistically indistinguishable from infinity. As it happens, this behavior applies to war. Many recent studies estimate $\alpha$s with values less than 2, and some scholars, like @braumoeller2019, infer from this that extinction-level wars are not just possible but probable, with a higher likelihood than many assume.

Second, in terms of practical challenges, most real-world data are not uniformly explained by the power-law. This fact is noted by @clausetEtAl2009 who recommend identifying a minimum value of $x$ such that the data is power-law distributed for all $x \geq x_\text{min}$. While this seems like a compromise, @clausetEtAl2007 argue that in many studies, power-law behavior in the extreme tail of a thick-tailed distribution is of greatest concern to researchers. Some data truncation is a necessary trade-off for ensuring an accurate fit for the power-law for the largest events in the data.

This practical concern is also relevant for studying war. Using the CoW dataset used to produce Figure 1, Figure 2 shows the optimal fit for a power-law model to the battle death series. Consistent with best-practice, the appropriateness of the power-law fit for data is shown using a log-log plot where the observed values of battle deaths are shown on the x-axis and the empirical CDF, $\Pr(X > x)$, is shown on the y-axis. Based on the form of the power-law model, in a log-log plot you should see a negative linear correlation between $x$ and $\Pr(X > x)$. Sure enough, this is what we observe in the data. This is with the caveat, however, that we observe power-law behavior for all large $x \geq x_\text{min}$. Specifically, the parameters for the power-law model are $\alpha = 1.525$ and $x_\text{min} = 7,061$. While the power-law slope seems to make for a good linear fit for more extreme wars and the size of the minimum war size where the power-law applies is small, because of the extreme distribution in war deaths nearly a majority of the wars in the data *cannot be explained* by the model (46.3% fall below $x_\text{min} = 7,061$).

```{r}
fig("fig2.png")
```

Most scholars who have used the power-law to study variation in the size of war accept some data loss as a reasonable trade-off for the ability to accurately fit the power-law for extreme conflicts. However, more recent work by @cunen2020 raises the concern that the conventional power-law model leaves substantial variation in war unexplained. Further, by removing observations, they worry that statistical precision is being needlessly sacrificed for the sake of making a particular model fit the data.

This loss in precision has special relevance for scholars studying war. Much of the recent up-surge in interest in examining macro trends in war size is driven in reaction to the best-selling book, *Better Angels of Our Nature*, by @pinker2011. One of Pinker's claims is that international wars have become less deadly over time. This argument, known as the decline of war thesis, is not original to Pinker [see @gaddis1986long], and a variety of studies have been published in the ensuing years that attempt to apply various statistical tests to assess its veracity. Most of these tests involve using the power-law.

Some prominent work in this vein fails to find a statistically significant difference in power-law models fit before and after World War II [@braumoeller2019] – the cut-point most scholars believe marks a decline in war's likelihood of occurrence and deadliness due to a rise in international institutions, the liberal international order, and nuclear deterrence. Other work yields similar conclusions [@clauset2017enduring; @clauset2018trends], but not all studies agree [@cunen2020]; @spagat2020decline\]. @cunen2020, who use an alternative model known as the inverse Burr, find evidence of a statistically detectable change in the chances of deadly wars in the second half of the 20th century. Most unique about their study is that, unlike work that relies on the classic power-law, their approach allows them to use all of the data in their conflict series to draw this conclusion, suggesting that the classic power-law might lack the statistical power to detect a change in war fatalities.

To demonstrate, Figure 3 shows the same data as presented in Figure 2. The difference is that the inverse Burr model, suggested by @cunen2020 as an alternative for the power-law, is fit to the data. The inverse Burr is more complex than the classic power-law, allowing it to accommodate power-law behavior in the extreme tail of the distribution while also explaining variation in smaller events that fall below the $x_\text{min}$ threshold required to fit the classic power-law to data.

```{r}
fig("fig3.png")
```

The inverse Burr model has three parameters: a scale parameter $\mu$, and two shape parameters, $\alpha$ and $\theta$. A good way to think about these parameters is that $\mu$ captures the central tendency of the data, $\alpha$ captures the density of the data leading up to its central tendency, and $\theta$ captures the density of extreme values to the right of the data's central tendency. Confusingly, the last parameter, $\theta$, is approximately analogous to the power-law slope $\alpha$.

The $\Pr(X > x)$ as specified by the inverse Burr is given as:

$$
\Pr(X > x) = 1 - \left[\frac{(x/\mu)^\theta}{1 + (x / \mu)^\theta} \right]^\alpha
$$ {#eq-invburr}

where $x$, $\mu$, $\alpha$, and $\theta$ are all strictly positive. For very large $x$, the fact that $\theta$ captures power-law behavior can be seen by the following approximate specification:

$$
\Pr(X > x) \approx \alpha (\mu / x)^\theta.
$$ {#eq-largeinvburr}

This functional form offers more flexibility, which is on display in Figure 3. Rather than being restricted to fitting a linear relationship between $x$ and $\Pr(X > x)$ in log-log space, the inverse Burr can fit a quasi-concave and monotone curve, allowing it to capture both small values of $x$ as well as very large values in a thick-tailed distribution.

While @cunen2020 tout the inverse Burr as an unproblematic alternative to the power-law, it under-predicts the likelihood of the most extreme wars in the CoW battle death series – a fact that is clearly visible in Figure 3. This is a point that others have raised [@braumoeller2021trends], but it is premature to dismiss the inverse Burr simply for this reason, because there is one additional promised benefit of the inverse Burr as opposed to the classic power-law that may off-set this limitation: its ability to model the distribution of $x$ using covariates.

In their study, @cunen2020 showcase this strength by parameterizing their inverse Burr model using both a binary indicator for the time period (before or after a particular cut-point) for the $\mu$ and $\theta$ parameters, as well as a continuous covariate (the average polity score among countries fighting a war) that is part of the $\mu$ parameter as well. In particular, they specify that:

$$
\mu_{L,i} = \mu_{L,0}\exp(\beta_L w_i) \quad \text{and} \quad \mu_{R,i} = \mu_{R,0}\exp(\beta_Rw_i)
$$ {#eq-cunenspec}

along with $\theta_L$, $\theta_R$ and a constant $\alpha$. The value $w_i$ represents average polity scores, and the $\beta$ parameters capture how democracy scores differently predict the central tendency $\mu$ before and after the specified cut-point.

The ability to incorporate covariates into the estimation of an inverse Burr model potentially off-sets its inferior performance in the extreme tail of the CoW conflict series. As @cunen2020 note, their parameterized model offers a superior fit for the data relative to a basic inverse Burr model that assumes constant $\mu$, $\alpha$, and $\theta$.

The value of this approach goes beyond providing a better fit with greater statistical precision. @cunen2020 are able to offer affirmative evidence consistent with the decline of war thesis, and provide substantive insight into the role of democracy in explaining variation in war size. First, using an innovative change-point algorithm, @cunen2020 identify 1950 as the best fitting cut-point in the CoW series. At that cut-point, the difference in inverse Burr fits before and after are statistically distinguishable, with the post-1950 data consistent with a decline in the chance of particularly large wars.

Second, @cunen2020 show that democracy has little correspondence with war size before 1950, but that democracy predicts smaller wars after 1950. This finding applies specifically to the central tendency $\mu$, and means that when the average democracy score among countries fighting a war increases, the scale of wars declines.

Cunen et al.'s finding with respect to democracy is truly unique and novel. But the real gift their approach offers to conflict scholars is their methodology for identifying this relationship. The ability to model war size, not just as a stationary series, or by fitting separate models to discrete periods, offers researchers greater flexibility in identifying nuanced relationships between factors of interest and the escalatory potential of war.

This, however, is where Cunen et al.'s [-@cunen2020] study ends and the contribution of this one begins. If a tool like the inverse Burr model is to have more wide-spread acceptance among conflict scholars interested in testing theories about war size, more guidance is required than @cunen2020 offer in their paper. While they do have an online appendix and make their R code available for implementing their method, Cunen et al. offer little in the way of practical guidance or a convenient user interface for applied researchers. This study is motivated by the desire to fill this gap by offering these resources to conflict scholars. This is the subject of the next section.

# Using the Inverse Burr to Study War

Like many innovations in statistics, the inverse Burr distribution was introduced at various points in different fields, and called by different names. @kleiber2008guide offers a concise history, noting in particular the equivalence between the inverse Burr, as it is known in actuarial science [@kleiber2003statistical; @klugman2012loss], and the Dagum distribution as it is known in some corners of economics where, for decades, it was mostly limited to non-English speaking journals. The genesis of the inverse Burr, or the Dagum distribution in particular, was the need to identify a better distributional model for income and wealth than either the log-normal or Pareto distributions. Unsatisfied with these options, in the 1970s Camilo Dagum [-@dagum1977anm] proposed a three parameter distribution, which would become his namesake, that blended the Pareto distribution's ability to handle thick-tailed data and the log-normal's ability to capture an interior mode, or peak in the density function [@kleiber2008guide]. The flexibility of the form that became the Dagum/inverse Burr model is such that it not only can handle thick-tailed data with an interior mode, but it can also capture zero-mode data (more akin to the classic power-law).

The impact that the three inverse Burr parameters have on the form of the density function is demonstrated in the next three figures (see @eq-burrdens for the density function). In each, one of the parameters is changed while holding the others fixed. This helps to show how an increase in each changes the mode of the data and the thickness of the distribution, both up to the mode and after it.

First, consider Figure 4. It shows how the density function of the inverse Burr changes for $x \in [0, 10]$ where $\mu \in \{0.5, 1, 2, 4 \}$ and $\alpha = \theta = 2$. For this particular arrangement of parameters, as $\mu$ increases, so does the mode, or peak, of the data's density distribution. In fact, holding the other parameters constant at 2, $\mu$ is approximately equivalent to the mode of the data.

```{r}
fig("fig4.png")
```

Now, consider how the density function changes with $\alpha$. Figure 5 is like the last, but it now shows the density function holding $\mu = \theta = 2$ while $\alpha \in \{0.5, 1, 2, 4\}$. As $\alpha$ increases, the mode does as well, but notice that the rate of increase is smaller. Also notice that the density of the left-hand side of the distribution shows far more variation. When $\alpha$ is small, the left-hand side is very thick, but as $\alpha$ increases the thickness of the left-hand side shrinks to zero for small $x$. In short, greater $\alpha$ means a lower chance of small events.

```{r}
fig("fig5.png")
```

Finally, consider $\theta$. Figure 6 shows how the density function changes holding $\mu = \alpha = 2$ and where $\theta = \{0.5, 1, 2, 4\}$. Notice that as $\theta$ increases a few interesting things happen. First, the mode increases, meaning the most likely event becomes larger. At the same time, for the highest $\theta$ the thickness of the right-hand side shrinks, meaning that the chances of very large events goes down. The same is true for the left-hand side of the distribution. Notice, however, that for the smallest value of $\theta$, not only does the mode approach 0, the distribution actually lacks an interior mode. The density at $x = 0$ is $\infty$. Further, the right-hand side has a much thicker tail, meaning the likelihood of very large $x$ is also higher. This behavior shows that the inverse Burr is capable of capturing not just power-law behavior in the extreme tail, but also across an entire variable if need be.

```{r}
fig("fig6.png")
```

These three examples do not begin to scratch the surface of the inverse Burr's flexibility, but they do offer a sample of what the three key parameters of the model imply for data. An increase to the scale parameter $\mu$ increases the mode of the data, an increase to the $\alpha$ parameter increases both the mode and shrinks the lower tail of the data, and an increase to $\theta$ increases the mode and shrinks the lower and upper tails of the data. This combination gives the inverse Burr a high degree of built-in flexibility, which is part of the rationale @cunen2020 offer for preferring it to the classic power-law for studying war. If a further set of examples were considered, one would quickly realize that the clean delineation between these parameters and the shape of the distribution is porous – due to the non-linear specification of the model.

This flexible combination also hints that modeling data using the inverse Burr requires attention to more than just the central tendency of the data. Camil Dagum's goal in producing the functionally equivalent Dagum distribution was to provide an appropriate fit for the whole of a distribution. He wanted to explain not just the position of the mode, but also the density of observations leading up to and following it.

As a consequence, parameterizing the inverse Burr with covariates requires paying close attention to how the model is specified and, then, to how its results are interpreted. As noted in @eq-cunenspec, @cunen2020 used both polity and a discrete pre/post indicator to specify not just one of the inverse Burr parameters, but two – $\mu$ and $\theta$. This means that Cunen et al. conditioned aspects ranging from the estimated mode to the thickness of the density distribution to its left and right on covariates of war size.

Such a goal is more comprehensive than is the norm in political science. It also has the potential to be somewhat confusing for those being newly introduced to this approach. Therefore, it will help to break this methodology down into discrete steps – specification, estimation, and evaluation – as it is introduced.

## Specification

The inverse Burr has three parameters, and each can be specified as a function of covariates or left as a constant. Modeling these parameters can be done by applying a non-linear transformation to a linear weighted sum of covariates, just like more common generalized linear models. @cunen2020 use the exponent in particular. The rationale is that this approach is the most practical. Each of the inverse Burr parameters is strictly positive, and using the exponent preserves this quality regardless of the values generated by the linear sum of model covariates.

Deviating somewhat from @cunen2020, a convenient way to express the specification of each of the parameters is as follows. For an inverse Burr model fit to a variable $x_i$, let the density distribution for this function be given as:

$$
\text{Burr}^{-1}(x_i; \mu_i, \alpha_i, \theta_i) = \frac{\alpha_i \theta_i(x_i / \mu_i)^{\alpha_i\theta_i}}{x_i[1 + (x_i / \mu_i)^{\theta_i}]^{\alpha_i + 1}} 
$$ {#eq-burrdens}

where each of the parameters is specified as the exponent of a weighted sum of model covariates:

$$
\begin{aligned}
\mu_i & = \exp(\mathbf W ' \mathbf \beta) \\
\alpha_i & = \exp(\mathbf Y ' \mathbf \delta) \\
\theta_i & = \exp(\mathbf Z'\mathbf \gamma)
\end{aligned}
$$ {#eq-paramspecs}

It is possible for the sets of covariates used to model these parameters to be identical, or completely different. These covariates may be either discrete or continuous, and each includes a constant.

Consider an applied example. Say a researcher wanted to condition the shape of the inverse Burr distribution for war deaths in the CoW conflict series on the basis of three factors: an indicator for the post-1950 period, the average of belligerent country polity scores at the time the war started, and the natural log of the pooled belligerent country populations at the time the war started.[^1] If one wanted to allow all three inverse Burr parameters to vary as a function of these factors, they might write:

[^1]: These variables originate from the Polity Project dataset [@marshalletal2017p] and population data comes from the National Military Capabilities dataset [@singer1987rcwd; @singeretal1972cdu].

$$
\begin{aligned}
\mu_i & = \exp[\beta_0 + \beta_1 \text{post}_i + \beta_2 \text{polity}_i + \beta_3 \log(\text{pop}_i)] \\
\alpha_i & = \exp[\delta_0 + \delta_1 \text{post}_i + \delta_2 \text{polity}_i + \delta_3 \log(\text{pop}_i)] \\
\theta_i & = \exp[\gamma_0 + \gamma_1 \text{post}_i + \gamma_2 \text{polity}_i + \gamma_3 \log(\text{pop}_i)]
\end{aligned}
$$ {#eq-warspec}

For analysis, it is possible to program such a regression model using software like R, which is the approach that @cunen2020 took in their own analysis. While the user could be left to either copy and augment the code made available by @cunen2020, this is a sub-optimal approach for most researchers using the R programming language. Most R users are initially trained in regression analysis using a specific user interface. Users give data to a function, like `lm()`, and represent the model they want to estimate by creating a formula object, using syntax like `outcome ~ var1 + var2`.

To bring the user interface for inverse Burr models more in line with this convention, the `{invburreg}` R package (still under development) was created.[^2] It makes it possible to specify an inverse Burr regression with all the specificity this kind of model requires using syntax that is mostly consistent with basic R modeling functions. The below code snippet offers an example where the model specified formally above is to be estimated in the R programming software.

[^2]: The development version of the package can be accessed here: <https://github.com/milesdwilliams15/invburreg/tree/main>.

```{r}
#| eval: false
#| echo: true

## open the package and access the wars example data
library(invburreg)
data("wars")

## fit an inverse Burr model to data
model_fit <- ibm(
  outcome = fat,
  mu = ~ post1950 + dem + pop,
  alpha = ~ post1950 + dem + pop,
  theta = ~ post1950 + dem + pop,
  data = wars
)

```

Teflecting the complexity of the inverse Burr model, this code uses a slightly modified syntax compared with the conventional approach. There are three parameters that can accommodate covariates, and so there are three places in the `ibm()` function to specify a parameter-specific formula. Unlike typical approaches, only the right-hand side of the formula is required. Since there can only be one outcome variable, the option `outcome` allows users to specify the outcome once so that it does not have to be repeated for each parameter-specific formula.

## Estimation

Over the years, many different methods for fitting the inverse Burr to data have been proposed. Early efforts relied on non-linear least squares, but today, maximum likelihood estimation (MLE) is the recommended approach [@kleiber2008guide]. @cunen2020 use MLE in their own analysis of the CoW conflict data.

Other technical resources exist if readers are interested in the specifics of MLE for the inverse Burr [@dey2017dagum], but a short summary is offered here. Given an outcome $x_i$, the optimal values of $\hat \mu_i$, $\hat \alpha_i$, and $\hat \theta_i$ are those that maximize (expressed in log-form):

$$
L = \sum_{i = 1}^n \log[\text{Burr}^{-1}(x_i; \hat \mu_i, \hat \alpha_i, \hat \theta_i)]
$$ {#eq-mle}

As with most MLE methods, a closed form solution does not exist, so a numerical optimizer is required. @cunen2020 use the method developed by @nelder1965simplex, which is also the same method used by the `{invburreg}` package.

While estimation is straightforward, inference for inverse Burr parameters is somewhat fraught. @cunen2020 opt to use the Hessian to calculate parameter standard errors, but in practice this does not always work. In trials done by this author, there are certain combinations of model specifications and predictors that yield an indefinite Hessian, even though the @nelder1965simplex method was able to identify a unique solution. For this reason, the `{invburreg}` package relies on bootstrapping to compute standard errors. The trade-off is that while bootstrapping offers a consistent and robust estimate of parameter variance, it can be computationally demanding.

## Summarizing Results

There are several ways to summarize the results for an inverse Burr regression. Two approaches are presented here. The first is by way of a regression table. This is the format that most journals in political science expect to see regression results presented, and it is trivial to do so for inverse Burr models. However, there are some important considerations to keep in mind.

For one, since there are three different parameters regression estimates may apply to, this will need to be signaled somehow in the table. It might be appropriate either to use row headings for model predictors, or to have multiple columns. In Table 1, the second approach is adopted. Results are shown for two different inverse Burr models. One is a *Baseline* model that includes no covariates, and the other is a *Covariate* model where a post-1950 indicator, average polity, and the log of pooled country population are used as predictors for all three inverse Burr parameters. There are six columns in total, two for each of these parameters with a label indicating which parameter estimates they are associated with. Note that the parameters are labelled as the log of their value. This is because the exponential transformation was used to transform the linear component of the model to ensure fitted parameter values are strictly positive.

```{r}
fig("fig_regtab.png")
```

While the presentation of the model in a regression table is more involved, many elements of the output remain familiar. Each cell entry is a coefficient estimate for the relevant parameter – $\alpha$, $\mu$, or $\theta$ – and standard errors are shown in parentheses with statistical significance indicated using stars.

Before turning to the next method of summarizing results, consider the estiamtes shown in Table 1. For this specification of the inverse Burr model for war fatalities, the only model parameter where covariates are statistically significant is $\alpha$. Recall from the previous section that this term captures both variation in the location of the mode and the thickness of the right-hand side of the distribution. An increase in $\alpha$ increases both the mode while reducing the likelihood of small events. Among the three factors included in the model, average democracy and pooled belligerent population are the only significant factors. Contrary to @cunen2020, the post-1950 indicator is not significant, though its sign is consistent with a lower mode and higher likelihood of small wars. Meanwhile, an increase in the pooled population of belligerents increases modal fatalities and reduces the chances of a smaller war. Surprisingly, democracy scores do as well, which conflicts with the findings of @cunen2020.

These conclusions are about as far as a regression table can take us. Because of the non-linear nature of the inverse Burr model, the results are better visualized. And as will be shown, visualizing the data also shows that the regression estimates can be somewhat misleading.

One useful approach is to simulate draws from the data holding covariates constant at quantities of interest, and then to show the results in a log-log plot similar to those presented in an earlier section. Figure 7 offers an example. Results for both the *Baseline* and the *Covariate* models are shown. For the latter, war deaths were similated holding covariates constant at their values for World War II. For reference, the empirical CDF for World War II is shown in the plot as well.

```{r}
fig("fig7.png")
```

The simulated inverse Burr distributions from these two models show that the *Covariate* model makes for a much better fit for World War II than the *Baseline* model. The former generates a distribution that substantially under predicts $\Pr(X > x)$ for World War II. Conversely, World War II could very easily have been drawn from the conditional inverse Burr distribution fit with covariates. This is one of the purported benefits of the inverse Burr model. It's built-in flexibility is enhanced by incorporating covariates.

This method of presenting model results can also be a good way to present the conditional impact of different covariates. It also can be a good way to further check intuitions about the actual estimated impact of a certain covariate. Because of the non-linear nature of the inverse Burr, significant predictors in one model parameter can still impact variation in other parts of the distribution. This fact is on display in Figure 8, which shows simulated CDFs of war deaths based on the *Covariate* model. Holding the other factors constant at their mean, the average polity of belligerents is allowed to vary from -10, to 0, to 10.

```{r}
fig("fig8.png")
```

The figure shows that the overall shape of the fatality distribution changes conditional on polity, and in somewhat counter-intuitive ways. Consistent with the inference made when discussing the results in Table 1, an increase in the average polity scores among belligerents increases the mode of the distribution and appears to reduce the likelihood of smaller wars. At the same time, the upper tail of the distribution becomes thinner, and substantially so, meaning that as the average polity score among belligerents increases, the likelihood of extremely huge wars declines. This is the kind of nuanced finding be-fitting a flexible model like the inverse Burr.

## A Word of Warning

The inverse Burr model can be a helpful tool in the study of war. The previous sections make this clear. It offers a flexible distributional form that is amenable to analysis in a regression framework, and with the right tools it is easy to summarize model results to show the substantive impact of certain covariates on war deaths.

There is one major limitation worth mentioning, however. The model is sensitive to the scale of covariates as well as the outcome. Consider Table 2, which shows the results for the same *Covariate* model presented in Table 1 and a similar model using the very same data, with the caveat that war fatalities have been rescaled. The results are different. Following @cunen2020, two changes were made to fatalities. First, since no war in the data can have less than 1,000 total deaths, the data is shifted downward so that the smallest war is now of size 1. Second, to add extra variation to smaller wars (there are several that clock in at 1,000) they are recoded to have values between 1 and 9. This transformation, while having no real impact on the ordering of the fatality data, does influence the fit of the inverse Burr model. Seemingly innocuous transformations like this can add up to dramatically different results.

```{r}
fig("fig_regtab2.png")
```

Because the inverse Burr can be so sensitive to the scaling of the outcome, any transformations that a researcher wants to apply should be clearly justified. Even then, it may be advisable to report estimates with and without this desired transformation to check whether, and by how much, it changes the results.

# Conclusion

An actuary that enters a war room would bring a whole-of-distribution approach to assessing the fatality risk of war. This makes for a richer analysis, one that can capture unexpected and important nuances in the impact of different factors on this risk. While @cunen2020 brought these tools to the attention of conflict scholars, their study offers little in the way of practical guidance or intuitions for applied researchers. The goal behind this study was to introduce readers to the relevant tools for doing this kind of analysis and how to use them.

The particular actuarial tool suggested by @cunen2020, the inverse Burr, has substantial build-in flexibility. This gives it the ability to fit a wide range of distributions with power-law tails of varying thickness and with either a zero or interior mode. It also can handle covariates in a regression framework, where the researcher is free to allow any of the inverse Burr's three parameters to be conditioned by these factors.

An R package called `{invburreg}` was introduced that offers an intuitive user interface for political scientists interested in using this method to analyze correlates of war deaths. It adopts many coding conventions common to other models in the R programming language.

In addition, ways of specifying inverse Burr models, details about model estimation and inference, and presentation of results were addressed. A word of warning was provided as well. One potential trade-off with the inverse Burr's flexibility is its sensitivity to the scaling of the outcome variable. Researchers should do their best to ensure that any data transformations they apply are justified, and they may want to show results both with and without their preferred transformations to assure readers that their results are insensitive to them.

Having a tool like the inverse Burr for studying sizes of wars is a timely and important issue. Since @pinker2011 published *Better Angels of Our Nature*, a number of scholars have taken up the gauntlet of assessing the so-called decline of war thesis. The literature that has emerged around this issue yields mixed results and has engendered disagreement over modelling and measurement strategies [@braumoeller2019; @cederman2003; @cedermanEtAl2011; @cirillo2016statistical; @clauset2017enduring; @clauset2018trends; @cunen2020 @spagat2020decline; @spagat2018fundamental; @weisiger2013logics]. While the method proposed by @cunen2020 has met with some skepticism [@braumoeller2021trends], there is great potential for the inverse Burr to play a more prominent role in this literature and the overall debate surrounding the decline of war. In particular, its ability to accommodate covariates offers the possibility of testing nuanced hypotheses about various factors at the systemic level of analysis as they relate to war's escalatory potential.

The results shown in this study are primarily for demonstration, but they also show how the inverse Burr supports a richer analysis. Using the CoW conflict series, the parameters of an inverse Burr model of war fatalities were allowed to vary as a function of a post-1950 indicator, average belligerent polity scores, and the log of the pooled belligerent populations. The results show, first, that the inclusion of covariates improves the inverse Burr's performance, particularly in explaining extreme wars like World War II. Second, they show that the expected distribution of war sizes is sensitive to factors like democracy and population. With respect to democracy in particular, an increase in average polity scores predicts an increase in the modal war size and a decline in the likelihood of small wars. At the same time, it predicts a decline in the chances of huge wars. This unique finding might help explain why the decline of war thesis as been so hard to peg down. Polity scores have generally increased over time, and they seem to predict contradictory trends in the chances of small versus large wars.

It is the hope that conflict scholars will be able to use this method and the associated R package to better explain variation in war size and test a wide range of old and new theories about war escalation. But this method need not be restricted to this narrow sub-field. Political scientists studying a range of issues may find the inverse Burr a helpful tool for solving similar data challenges where easy solutions did not exist.

# References
