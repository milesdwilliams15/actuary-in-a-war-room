---
title: "An Actuary Walks into a War Room: Using a Model from Actuarial Science to Predict War Size"
bibliography: "ref.bib"
author: Miles D. Williams^[Denison University, Granville, OH USA. Contact at mdwilliams@denison.edu.]
date: "`r Sys.Date()`"
abstract: "**Abstract**: This document was produced using a Quarto (.qmd) template. It's goal is to provide a better looking and functioning Word template for researchers to use when weaving together reports with their text and embedded code in Quarto."
format: 
  docx:
    reference-doc: word_docx_new_template.docx
    number-sections: false
    number-depth: 3
    fig-dpi: 500
    fig-align: center
execute: 
  echo: false
  warning: false
  message: false
---

***Keywords***: Conflict, Battle Deaths, Inverse Burr

# Introduction

Risk is the business of actuaries, and when it comes to war business is good. Actuaries typically work for insurance companies or firms, and their job is to model financial risk and ruin. That means they care about more than a single point estimate of the expected value of, say, an insurance claim. They care about the probability distribution over the possible claims the insured might make. Often, these claims follow a thick tailed distribution where the likelihood of extreme values is higher than you would see with more conventional probability density functions. Because insurance companies and firms want to avoid financial ruin, they take the odds of extreme events seriously and want their actuaries to use the most rigorous methods possible for modeling this risk.

When @cunen2020 recently used the inverse Burr distribution to model the sizes of international wars, they drew from the deep well of actuarial science. They even used a statistical package created specifically for the purpose of doing actuarial science – an R package called `{actuar}` – to apply the inverse Burr to war fatality data. What does it mean to approach the study of war through the lens of an actuary? What theoretical framework and assumptions are implied? @cunen2020 say very little about the actuarial roots of their methodology. This is a missed opportunity to showcase the potential on offer with Cunen et al.'s approach – as well as the pitfalls.

This study is motivated by the goal of better contextualizing the use of the inverse Burr for studying the risk of deadly wars and providing guidance and resources for researchers interested in implementing this method. The study proceeds in three parts.

First, the problem @cunen2020 turned to actuarial science to solve – the heavy-tailed distribution of war fatalities – is discussed. This problem has posed a technical challenge and, thus, been a source of contention among scholars interested in testing macro trends in war's deadliness. Cunen et al.'s work is a recent contribution to a small but growing literature centered on the decline of war theory. Much of this literature has, in part, developed in response to the best-selling book, *The Better Angels of Our Nautre,* by Steven @pinker2011. The merits of assessing the risk of deadly wars are self-evident, and by extension, so are the merits of micro-founding and explaining methods used to make this assessment.

Second, readers are introduced to the inverse Burr model and how it can be used in conflict research. When @cunen2020 applied the model to the problem of modeling war size, they did not just borrow it from actuarial science, they also innovated by showing that the model could be parameterized with covariates. However, their novel approach lacks a consistent and easily accessible user interface for researchers, or a full explanation of what this actuarial tool implies for this methods appropriate use and evaluation. To facilitate better accessibility, a new statistical package developed in the R programming language is introduced that allows users to execute an inverse Burr regression model and summarize the results in ways consistent with standard modeling functions in the R language. However, because of the unique purpose for which the inverse Burr was intended, assessing its output requires a different approach than most political scientists are accustomed to. The model is ideal, not for generating a unique prediction for war size, but a conditional distribution of possible war sizes.

As the world is witnessing an upsurge in conflicts, the need for appropriate analytical tools and the right intuitions for how to use them is timely and necessary. Quantifying the risk of conflict escalation is important not just to academics, but to policy-makers as well. @cunen2020 introduced a helpful tool to conflict research for achieving this goal, but for it to be of greatest utility to the field, it is essential that researchers have guidance on how to use it and convenient tools for doing so.

# War, the Power-law, and the Alternative

One of the foundational questions in the quantitative study of war centers on variation in war deaths. Why are some wars short-lived and result in only a few thousand fatalities, while a others last years (even decades) and kill millions? [@richardson1948; -@richardson1960] was among the first to approach this question using data. He was preceded in this effort by data collection done by @wright1942study, but Richardson was the first to bring "real statistical acumen," as Braumoeller [-@braumoeller2021trends, 274] put it, to the data Wright produced. One of the key observations Richardson made was the fact that many wars are quite small while only a few become especially deadly. In his 1948 work, Richardson noted that the spread of war deaths displays power-law like behavior, meaning that the wars that are the most deadly are exceptionally huge.

This observation is preserved in more recent and higher quality datasets such as the war series produced by the Correlates of War (CoW) project [@sarkeeswayman2010rw]. Figure 1 shows the variation in total battle deaths recorded per war in the CoW data by the first year the war started. The data documents 95 international wars from 1816 to 2007, and, just like Richardson observed decades ago, this more recent dataset shows that most of the wars fought during the past two centuries were generally small. But a few were enormous. World War II, the deadliest war in the CoW dataset, had over 16 million recorded battle deaths. This makes up about 51.8% of all battle deaths in the data. World War I and World War II combined make up 78.6% of all battle deaths, and the top 10 deadliest wars account for nearly 95% of all battle deaths. This kind of distribution is far more extreme than a typical Pareto distribution where 20% of observations are responsible for 80% of outcomes.

```{r}
fig <- function(name) {
  knitr::include_graphics(
    here::here("_figs", name)
  )
}
fig("fig1.png")
```

Data that is prone to the kind of massive up-swings in magnitude seen in war require specialized statistical tools to analyze. While methods differ a bit, most recent studies use the classic power-law model to study variation in war size [@braumoeller2019; @cederman2003; @cedermanEtAl2011; @cirillo2016statistical; @clauset2017enduring; @clauset2018trends; @spagat2020decline; @spagat2018fundamental]. The model's name takes inspiration from its functional form, which specifies that the likelihood of seeing an event greater than size $x$ is inversely proportional to the size of the event raised to the power $\alpha > 0$:

$$
\Pr(X > x) \propto 1 / x^\alpha
$$ {#eq-powerlaw}

The parameter $\alpha$, what can also be called the power-law slope, determines the likelihood of extreme events. Bigger values mean the likelihood of extreme events is lower, while smaller values mean the likelihood of extreme events is higher.

In practice, the power-law form has some unique behaviors and poses some challenges for fitting it to real world data. First, in terms of unique behaviors, one of the most consequential is that if the $\alpha$ parameter is less than or equal to 3, finite variance for the variable $x$ cannot be guaranteed, and if $\alpha$ is less than or equal 2, a finite mean cannot be identified. In short, depending on how extreme the distribution of the variable $x$ is, a power-law fit for the data may yield a result that says the expected magnitude of the variable is statistically indistinguishable from infinity. This issue is relevant for war. Many recent studies estimate $\alpha$s with values less than 2, and some scholars, like @braumoeller2019, infer from this that extinction-level wars are not just possible but probable, with a higher likelihood than many assume.

Second, in terms of practical challenges, most real-world data are not uniformly explained by the power-law. This fact is noted by @clausetEtAl2009 who recommend identifying a minimum value of $x$ such that the data is power-law distributed for all $x \geq x_\text{min}$. While this seems like a compromise, @clausetEtAl2007 argue that in many studies, power-law behavior in the extreme tail of a thick-tailed distribution is of greatest concern to researchers. Some data truncation is a necessary trade-off for ensuring an accurate fit for the power-law model for the largest events in the data.

This practical concern is also relevant for studying war. Using the CoW dataset used to produce Figure 1, Figure 2 shows the optimal fit for a power-law model to the battle death series. Consistent with best-practice, the appropriateness of the power-law fit for data is shown using a log-log plot where the observed values of battle deaths are shown on the x-axis and the empirical CDF, $\Pr(X > x)$, is shown on the y-axis. Based on the form of the power-law model, in a log-log plot you should see a negative linear correlation between $x$ and $\Pr(X > x)$. Sure enough, this is what we observe in the data. This is with the caveat, however, that we observe power-law behavior for all large $x \geq x_\text{min}$. Specifically, the parameters for the power-law model are $\alpha = 1.525$ and $x_\text{min} = 7,061$. While the power-law slope seems to make for a good linear fit for more extreme wars and the size of the minimum war size where the power-law applies is small, because of the extreme distribution in war deaths nearly a majority of the wars in the data *cannot be explained* by the model (46.3% fall below $x_\text{min} = 7,061$).

```{r}
fig("fig2.png")
```

Most scholars who have used the power-law to study variation in the size of war accept data loss as a reasonable trade-off for the ability to accurately fit the power-law for extreme conflicts. However, more recent work by @cunen2020 raises the concern that the conventional power-law model leaves substantial variation in war unexplained. Further, by removing observations, they worry that statistical precision is being needlessly sacrificed as well.

This loss in precision has special relevance for scholars studying war. Much of the recent up-surge in interest in examining macro trends in war size is driven in reaction to the best-selling book, *Better Angels of Our Nature*, by @pinker2011. One of Pinker's claims is that international wars have become less deadly over time. This argument, known as the decline of war thesis, is not original to Pinker [see @gaddis1986long], and a variety of studies have been published in the ensuing years that attempt to apply various statistical tests to assess its veracity. Most of these tests involve using the power-law. Some prominent work in this vein fails to find a statistically significant difference in power-law models fit before and after World War II [@braumoeller2019] – the cut-point most scholars believe begets a decline in war's likelihood of occurrence and deadliness due to a rise in international institutions, the liberal international order, and nuclear deterence. But @cunen2020, who use an alternative model known as the inverse Burr, do find evidence of a statistically detectable change in the chances of deadly wars in the second half of the 20th century. Unlike work that relies on the classic power-law, their approach allows them to use all of the data in their conflict series to draw this conclusion.

To demonstrate, Figure 3 shows the same data as presented in Figure 2. The difference is that the inverse Burr model suggested by @cunen2020 as an alternative for the power-law is fit to the data. The inverse Burr is more complex than the classic power-law, allowing it to accommodate power-law behavior in the extreme tail of the distribution while also explaining variation in smaller events that fall below the $x_\text{min}$ threshold required to fit the classic power-law to data.

```{r}
fig("fig3.png")
```

The inverse Burr model has three parameters: a scale parameter $\mu$, and two shape parameters, $\alpha$ and $\theta$. A good way to think about these parameters is that $\mu$ captures the central tendency of the data, $\alpha$ captures the density of the data leading up to its central tendency, and $\theta$ captures the density of extreme values to the right of the data's central tendency. Confusingly, the last parameter, $\theta$, is approximately analogous to the power-law slope $\alpha$.

The $\Pr(X > x)$ as specified by the inverse Burr is given as:

$$
\Pr(X > x) = 1 - \left[\frac{(x/\mu)^\theta}{1 + (x / \mu)^\theta} \right]^\alpha
$$ {#eq-invburr}

where $x$, $\mu$, $\alpha$, and $\theta$ are all strictly positive. For very large $x$, the fact that $\theta$ captures power-law behavior can be seen by the following approximate specification:

$$
\Pr(X > x) \approx \alpha (\mu / x)^\theta.
$$ {#eq-largeinvburr}

This functional form offers more flexibility, which is on display in Figure 3. Rather than being restricted to fitting a linear relationship between $x$ and $\Pr(X > x)$ in log-log space, the inverse Burr can fit a quasi-concave and monotone curve, allowing it to capture both small values of $x$ as well as very large values in a thick-tailed distribution.

While @cunen2020 tout the inverse Burr as an unproblematic alternative to the power-law, it under-predicts the likelihood of the most extreme wars in the CoW battle death series – a fact that is clearly visible in Figure 3. This is a point that others have raised [@braumoeller2021trends], but it is premature to dismiss the inverse Burr simply for this reason. One additional promised benefit of the inverse Burr as opposed to the classic power-law is its ability to model the distribution of $x$ using covariates.

In their study, @cunen2020 showcase this strength by parameterizing their inverse Burr model using both a binary indicator for the time period (before or after a particular cut-point) for the $\mu$ and $\theta$ parameters, as well as continuous covariate (the average polity score among countries fighting a war) that is part of the $\mu$ parameter as well. In particular, they specify that:

$$
\mu_{L,i} = \mu_{L,0}\exp(\beta_L w_i) \quad \text{and} \quad \mu_{R,i} = \mu_{R,0}\exp(\beta_Rw_i)
$$ {#eq-cunenspec}

along with $\theta_L$, $\theta_R$ and a constant $\alpha$. The value $w_i$ represents average polity scores, and the $\beta$ parameters capture how democracy scores differently predict the central tendency $\mu$ before and after the specified cut-point.

The ability to incorporate covariates into the estimation of an inverse Burr model potentially off-sets its inferior performance in the extreme tail of the CoW conflict series. As @cunen2020 note, their parameterized model offers a superior fit for the data relative to a basic inverse Burr model that assumes constant $\mu$, $\alpha$, and $\theta$.

The value of this approach goes beyond providing a better fit with greater statistical precision. @cunen2020 are able to offer affirmative evidence consistent with the decline of war theory, and provide substantive insight into the role of democracy in explaining variation in war size. First, using an innovative change-point algorithm, @cunen2020 identify 1950 as the best fitting cut-point in the CoW series. At that cut-point, the difference in inverse Burr fits before and after are statistically distinguishable, with the post-1950 data consistent with a decline in the chance of particularly large wars.

Second, @cunen2020 show that democracy has little correspondence with war size before 1950, but that democracy predicts smaller wars after 1950. This finding applies specifically to the central tendency $\mu$, and means that when the average democracy score among countries fighting a war increases, the scale of wars declines.

Cunen et al.'s finding with respect to democracy is truly unique and novel. But the real gift their approach offers to conflict scholars is their methodology for identifying this relationship. The ability to model war size, not just as a stationary series, or by fitting separate models to discrete periods, offers researchers greater flexibility in identifying more nuanced relationships between factors of interest and the escalatory potential of war.

This, however, is where their study ends and the contribution of this one begins. If a tool like the inverse Burr model is to have more wide-spread acceptance among conflict scholars interested in testing theories about war size, more guidance is required than @cunen2020 offer in their paper. While they do have an online appendix and make their R code available for implementing their method, Cunen et al. offer little in the way of practical guidance or a convenient user interface for applied researchers. This study is motivated by the desire to fill this gap by offering these resources for conflict scholars. This is the subject of the next section.

```{r}
fig("fig4.png")
```
